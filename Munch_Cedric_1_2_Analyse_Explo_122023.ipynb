{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "YPWPsLTA6PHzzu8NBJiBGc",
     "report_properties": {
      "rowId": "bQVdch3WwH71PERijuXOJf"
     },
     "type": "MD"
    },
    "id": "BVDmPdkgiwsk"
   },
   "source": [
    "# PROJET 10 DATA ANALYST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "k48auDisTvRZsZfKvxjypF",
     "report_properties": {
      "rowId": "oLZfdyjY97Qfpmb939g74j"
     },
     "type": "MD"
    },
    "id": "SuuBTKk3iwso"
   },
   "source": [
    "# OBJECTIF DE CE NOTEBOOK\n",
    "\n",
    "Pour l'Organisation Nationale de lutte Contre le Faux-Monnayage (ONCFM), nous devons produire :\n",
    "\n",
    "- Une analyse descriptive des données, notamment la répartition des dimensions des billets, le nombre de vrais / faux billets, etc.\n",
    "- Une détection automatisée des faux billets à partir des dimensions de ces derniers. Les méthodes à utiliser sont la régression logistique et k-means avec une matrice de confusion pour évaluer les performances des modèles. Une fois la phase d'entrainement et de test achevée, l'algorithme devra être capable de prédire si un billet est vrai ou faux.\n",
    "\n",
    "Glossaire :\n",
    "- diagonal : la diagonale du billet (en mm)\n",
    "- height_left : la hauteur du billet (mesurée sur le côté gauche, en mm)\n",
    "- height_right : la hauteur du billet (mesurée sur le côté droit, en mm)\n",
    "- length : la longueur du billet (en mm)\n",
    "- margin_low : la marge entre le bord inférieur du billet et l'image de celui-ci (en mm)\n",
    "- margin_up : la marge entre le bord supérieur du billet et l'image de celui-ci (en mm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "huuuZ9nVEfTyGWBHbQ8VgQ",
     "report_properties": {
      "rowId": "KcHwtHqP2KarZqESgS5Cuv"
     },
     "type": "MD"
    },
    "id": "bUeH7jbOiwss"
   },
   "source": [
    "## Etape 1 - Importation des librairies et chargement des fichiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "WUgzpCrtz6qvTWeMcwleCe",
     "report_properties": {
      "rowId": "QPTiDox9sGkHkQFnuuMVVz"
     },
     "type": "MD"
    },
    "id": "pCmIR_wtiwsu"
   },
   "source": [
    "## 1.1 - Importation des librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "PBnfkRJEuh40fJxLPsn3ZG",
     "report_properties": {
      "rowId": "LEAuwO1pfiygv0Qy8MKwvF"
     },
     "type": "CODE"
    },
    "executionInfo": {
     "elapsed": 3428,
     "status": "ok",
     "timestamp": 1688211938806,
     "user": {
      "displayName": "Tehaniii",
      "userId": "03067073566435797809"
     },
     "user_tz": -120
    },
    "id": "tGnFLpXziwsv",
    "outputId": "504834a0-ff06-49ab-b51d-bef4e9907a22"
   },
   "outputs": [],
   "source": [
    "#Importation des librairies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "import scipy.stats as ss\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chargement de la librairie graphique\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "sepiokARAir2xuLksAdLsK",
     "report_properties": {
      "rowId": "jsLdri6gPJ7yy6O3enpU4e"
     },
     "type": "MD"
    },
    "id": "1VxbQzPEiwsw"
   },
   "source": [
    "## 1.2 - Chargement du fichier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "eKALxcS4HU8fox5dd9w3Ha",
     "report_properties": {
      "rowId": "F6iF3THpMlmGFJP1AGoMzN"
     },
     "type": "CODE"
    },
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1688211938808,
     "user": {
      "displayName": "Tehaniii",
      "userId": "03067073566435797809"
     },
     "user_tz": -120
    },
    "id": "YPfRBY_Eiwsy"
   },
   "outputs": [],
   "source": [
    "#Importation du fichier population.csv en mettant l'index sur 'Zone'\n",
    "billet = pd.read_csv('./Data_source/billets.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Affichage des dimensions et de leurs types\n",
    "display(billet.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons voir que sur les 1500 lignes, la variable 'Height_left' contient des valeurs manquantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Affichage d'un échantillon\n",
    "display(billet.sample(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etape 2 - Analyse exploratoire des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 - Statistiques descriptives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Affichage des statistiques descriptives\n",
    "stats_descr = billet.describe().round(2)\n",
    "display(stats_descr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons remarquer 37 valeurs manquantes dans la variable 'margin_low'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 - Analyse univariée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Affichage des histogrammes avec la densité de probabilité\n",
    "for col in stats_descr.columns:\n",
    "    mu = stats_descr.loc['mean', col]\n",
    "    sigma = stats_descr.loc['std', col]\n",
    "    #Règle de Sturges pour déterminer approximativement le nombre optimal de classes\n",
    "    num_bins = int(np.ceil(np.log2(stats_descr.loc['count', col])) + 1)\n",
    "    #num_bins = 15\n",
    "    print(num_bins)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "    #Affichage de l'histogramme\n",
    "    n, bins, patches = ax.hist(billet[col], num_bins, density=True)\n",
    "\n",
    "    #Affichage de la densité de probabilité\n",
    "    y = ((1 / (np.sqrt(2 * np.pi) * sigma)) *\n",
    "         np.exp(-0.5 * (1 / sigma * (bins - mu))**2))\n",
    "    ax.plot(bins, y, '--')\n",
    "    ax.set_xlabel('Valeurs')\n",
    "    ax.set_ylabel('Densité de probabilité')\n",
    "    ax.set_title(f\"Distribution de '{col}' et densité de probabilité : \"\n",
    "                 fr'$\\mu={mu:.2f}$, $\\sigma={sigma:.2f}$')\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    #Test de Kolmogorov-Smirnov\n",
    "    print('Si p-value est inférieure à 0.05 alors on rejette H0 = normalité: {}\\n\\n'.format(ss.kstest(billet[col], 'norm')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les densités de probabilité des variables 'margin_low' et 'length' sont éloignées d'une loi normale principalement à cause de la présence des faux billets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Affichage des boxplots\n",
    "for col in stats_descr.columns:\n",
    "    sns.boxplot(data=billet[col], orient='h')\n",
    "    \n",
    "    plt.title(f\"Boxplot de '{col}'\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous ne remarquons pas de valeurs aberrantes dans les variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 - Analyse bivariée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Séparation en deux DataFrames avec et sans NA sur la variable 'margin_low'\n",
    "billet_isna = billet.loc[billet['margin_low'].isna(), :].copy()\n",
    "billet_dropna = billet.dropna().copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Matrice de corrélation entre les variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Matrice des corrélations utilisant le coefficient de corrélation de Pearson\n",
    "corr_matrix = billet_dropna.iloc[:, 1:].corr(method='pearson', min_periods=20)\n",
    "\n",
    "#Masque pour la partie triangulaire supérieure de la matrice\n",
    "mask = np.triu(corr_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Heatmap représentant la matrice des corrélations\n",
    "plt.figure(figsize=(15,6))\n",
    "plt.title(\"Heatmap des coefficients de corrélation de Pearson entre les variables\", fontsize=14)\n",
    "\n",
    "sns.heatmap(corr_matrix, annot=True, vmin=-1, vmax=1, cmap='coolwarm', mask=mask, fmt='.2f')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons voir que les variables 'length' et 'is_genuine' sont très fortement corrélées (0.85). Ce qui serait une piste pour la détection des faux billets à l'aide de la regression logistique..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pairplots pour visualiser les potentielles corrélations\n",
    "sns.pairplot(billet_dropna, hue='is_genuine', corner=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 - Imputation des valeurs manquantes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4.1 - Séparation des données en train et test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sélection des variables explicatives et de la variable à expliquer\n",
    "y = billet_dropna['margin_low']\n",
    "X = billet_dropna.drop(['is_genuine', 'margin_low'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Séparation des données en train et test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4.2 - Détermination du nombre optimal de variables explicatives"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#Création de l'espace de cross-validation\n",
    "folds = KFold(n_splits = 4, shuffle = True, random_state = 0)\n",
    "\n",
    "#Définition du dictionnaire des variables explicatives à tester\n",
    "hyper_params = [{'n_features_to_select': list(range(1, 6))}]\n",
    "\n",
    "#Instanciation du modèle et de la Recursive Feature Elimination (RFE)\n",
    "lm = LinearRegression()\n",
    "rfe = RFE(lm)\n",
    "\n",
    "#Instanciation de la GridSearchCV\n",
    "model_cv = GridSearchCV(estimator = rfe,\n",
    "                        param_grid = hyper_params,\n",
    "                        scoring= 'r2',\n",
    "                        cv = folds,\n",
    "                        verbose = 1,\n",
    "                        return_train_score=True)\n",
    "\n",
    "#Fit de la GridSearchCV\n",
    "model_cv.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#Résultats de la GridSearchCV\n",
    "cv_results = pd.DataFrame(model_cv.cv_results_)\n",
    "\n",
    "#Sélection des colonnes utiles\n",
    "cols = [i for i in cv_results.columns if not i.startswith('split') and not i.endswith('time')]\n",
    "cv_results = cv_results.loc[:, cols]\n",
    "\n",
    "display(cv_results)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Affichage de la contribution des variables explicatives à la performance du modèle\n",
    "plt.figure(figsize=(15,5))\n",
    "\n",
    "plt.plot(cv_results[\"param_n_features_to_select\"], cv_results[\"mean_test_score\"])\n",
    "plt.plot(cv_results[\"param_n_features_to_select\"], cv_results[\"mean_train_score\"])\n",
    "\n",
    "plt.xlabel('Nombre de variables explicatives')\n",
    "plt.ylabel('R-squared')\n",
    "plt.title('Contribution des variables explicatives à la performance du modèle')\n",
    "plt.legend(['validation score', 'train score'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le nombre optimal de variables explicatives retenu est 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4.3 - Détermination des variables explicatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choix du nombre optimal de variables explicatives et instanciation du modèle\n",
    "n_features_optimal = 2\n",
    "lm = LinearRegression()\n",
    "\n",
    "# TODO : faire un RFECV sur X_train et y_train ?\n",
    "#Instanciation de la Recursive Feature Elimination (RFE) et fit\n",
    "rfe = RFE(lm, n_features_to_select=n_features_optimal)\n",
    "rfe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Affichage de la liste des variables explicatives sélectionnées (True) et non sélectionnées (False) ainsi que leur rang\n",
    "print(list(zip(X_train.columns,rfe.support_,rfe.ranking_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Les variables explicatives sélectionnées sont 'length' et 'margin_up'.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4.4 - Validation de la régression linéaire sur X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Construction du train set avec les variables explicatives sélectionnées\n",
    "X_train_2feat = X_train[['length', 'margin_up']].copy()\n",
    "X_train_2feat = sm.add_constant(X_train_2feat)\n",
    "\n",
    "#Instanciation et entrainment du modèle\n",
    "model = sm.OLS(y_train, X_train_2feat).fit()\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Validation du modèle sur le test set\n",
    "y_pred_test = model.predict(sm.add_constant(X_test[['length', 'margin_up']]))\n",
    "\n",
    "#Calcul du R2\n",
    "r2 = r2_score(y_test, y_pred_test)\n",
    "\n",
    "print(round(r2, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le R^2 sur le Train Set est comparable à celui sur le Test Set et nous avons une corrélation moyenne (0.45)..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4.5 - Test des conditions de validité de la régression linéaire\n",
    "\n",
    "\n",
    "**Vérification des hypothèses :**\n",
    "\n",
    "- Normalité : Les erreurs résiduelles doivent être distribuées normalement. Cela signifie que les résidus doivent suivre une distribution normale avec une moyenne de zéro.\n",
    "- Homoscédasticité : L'homoscédasticité signifie que la variance des erreurs résiduelles est constante à tous les niveaux de la variable prédite.\n",
    "- Multicolinéarité : Cette hypothèse concerne la relation entre les variables prédictives (ou indépendantes) dans notre modèle de régression. Elle stipule qu'il ne devrait pas y avoir de forte corrélation (>0.6) linéaire entre les variables indépendantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extraction des valeurs prédites sur le train set\n",
    "y_pred_train = model.fittedvalues\n",
    "\n",
    "#Extraction des résidus sur le train set\n",
    "residues_train = model.resid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**- Normalité des résidus**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Définition de la moyenne et de l'écart-type des résidus\n",
    "mu = residues_train.mean()\n",
    "sigma = residues_train.std()\n",
    "\n",
    "#Règle de Sturges pour déterminer approximativement le nombre optimal de classes\n",
    "num_bins = int(np.ceil(np.log2(residues_train.count())) + 1)\n",
    "\n",
    "print(num_bins)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "#Affichage de l'histogramme\n",
    "n, bins, patches = ax.hist(residues_train, num_bins, density=True)\n",
    "\n",
    "#Affichage de la densité de probabilité\n",
    "y = ((1 / (np.sqrt(2 * np.pi) * sigma)) *\n",
    "     np.exp(-0.5 * (1 / sigma * (bins - mu))**2))\n",
    "ax.plot(bins, y, '--')\n",
    "ax.set_xlabel('Valeurs')\n",
    "ax.set_ylabel('Densité de probabilité')\n",
    "ax.set_title(f\"Distribution des résidus et densité de probabilité : \"\n",
    "             fr'$\\mu={mu:.2f}$, $\\sigma={sigma:.2f}$')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Nous ne sommes pas trop éloigné d'une distribution normale..."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Test statistique non-paramétrique de comparaison des distributions (bilatère) : Kolmogorov-Smirnov\n",
    "- Hypothèse nulle H0 : la densité de probabilité suit une loi normale\n",
    "- Hypothèse alternative H1 : la densité de probabilité ne suit pas une loi normale\n",
    "- Seuil alpha de rejet de H0 : 5%"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#Test de Kolmogorov-Smirnov\n",
    "print('Test de Kolmogorov-Smirnov : {}\\n\\n'.format(ss.kstest(residues_train, 'norm')))"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**- Homoscédasticité des résidus**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calcul des résidus standardisés sur le train set\n",
    "train_residuals_abs_sqrt=np.sqrt(np.abs(residues_train))\n",
    "\n",
    "#Affichage des résidus en fonction des valeurs prédites sur le train set\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.regplot(x=y_pred_train, y=train_residuals_abs_sqrt,\n",
    "            scatter=True,\n",
    "            lowess=True,\n",
    "            line_kws={'color': 'red', 'lw': 1, 'alpha': 0.8})\n",
    "\n",
    "plt.title('Racine carrée des valeurs absolues des résidus en fonction des valeurs prédites')\n",
    "plt.ylabel(\"Standarized residuals\")\n",
    "plt.xlabel(\"Fitted value\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La variance des résidus est \"plutôt\" constante à tous les niveaux de la variable prédite."
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#Création d'un DataFrame avec les valeurs prédites et une discrétisation en 3 groupes\n",
    "y_pred_train_grouped = pd.DataFrame({'y_pred_train': y_pred_train})\n",
    "y_pred_train_grouped['group'] = pd.qcut(y_pred_train_grouped['y_pred_train'], q=3, labels=False)\n",
    "\n",
    "#Ajout des résidus standardisés à la DataFrame\n",
    "y_pred_residues_train_grouped = pd.merge(y_pred_train_grouped, pd.DataFrame({'residues_train': train_residuals_abs_sqrt}), left_index=True, right_index=True, how='inner')\n",
    "\n",
    "display(y_pred_residues_train_grouped)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Moins de 5% d'Outliers par groupe.\n",
    "\n",
    "Test statistique paramétrique de comparaison (bilatère) : Levene\n",
    "- Hypothèse nulle H0 : var0 = var1 = var2\n",
    "- Hypothèse alternative H1 : au moins une variance diffère\n",
    "- Seuil alpha de rejet de H0 : 5%\n",
    "\n",
    "Conditions de validité :\n",
    "- Si les échantillons à comparer ont une distribution non-normale, choisir le paramètre center='median'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#Test de Levene\n",
    "print('Test de Levene :', ss.levene(y_pred_residues_train_grouped.loc[y_pred_residues_train_grouped['group'] == 0,'residues_train'],\n",
    "                                    y_pred_residues_train_grouped.loc[y_pred_residues_train_grouped['group'] == 1, 'residues_train'],\n",
    "                                    y_pred_residues_train_grouped.loc[y_pred_residues_train_grouped['group'] == 2, 'residues_train'], center='median'))"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**- Multicolinéarité**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calcul des VIF pour les variables explicatives\n",
    "vif = pd.DataFrame()\n",
    "vif[\"Feature\"] = X_train_2feat.columns\n",
    "vif[\"VIF\"] = [variance_inflation_factor(X_train_2feat.values, i) for i in range(X_train_2feat.shape[1])]\n",
    "\n",
    "display(vif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une valeur de VIF entre 1 et 5 est considérée comme acceptable (on ne prend pas en considération 'const' -> Y-intercept). Nous pouvons voir que les variables explicatives sont peu corrélées entre elles..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4.6 - Prediction des valeurs manquantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prédiction des valeurs manquantes à l'aide du modèle\n",
    "billet_isna['margin_low'] = model.predict(sm.add_constant(billet_isna[['length', 'margin_up']])).round(2)\n",
    "\n",
    "display(billet_isna.sample(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etape 3 - Sauvegarde des données"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#Concaténation des DataFrames avec et sans NA sur la variable 'margin_low'\n",
    "billet_compl = pd.concat([billet_isna, billet_dropna], axis=0, ignore_index=True)\n",
    "billet_compl['is_genuine'].replace([False, True], [0, 1], inplace=True)\n",
    "\n",
    "display(billet_compl.describe().round(2))\n",
    "display(billet_compl.sample(10))"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#Sauvegarde du fichier\n",
    "billet_compl.to_csv('./Data_transformed/billets_compl.csv', sep=';', index=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "datalore": {
   "base_environment": "default",
   "computation_mode": "JUPYTER",
   "package_manager": "pip",
   "packages": [],
   "report_row_ids": [
    "bQVdch3WwH71PERijuXOJf",
    "oLZfdyjY97Qfpmb939g74j",
    "8JGXqNfVG3AzH01bTCInW4",
    "KcHwtHqP2KarZqESgS5Cuv",
    "QPTiDox9sGkHkQFnuuMVVz",
    "LEAuwO1pfiygv0Qy8MKwvF",
    "jsLdri6gPJ7yy6O3enpU4e",
    "nCj19OueZbR8bwn0vBz6j0",
    "F6iF3THpMlmGFJP1AGoMzN",
    "ywlTo6eEX0CYCw5UEsKWyA",
    "PntplFQPhsmftLSzzsikTV",
    "Cpv1XiURwQ7SEzwguwskxE",
    "4BduE0kzP5a3gT6ujRenK8",
    "cQbD34mRlQ1IHrd8nj2Gnc",
    "awm72cPzvtK0sDe0qrZf76",
    "8j0lmVf3dm7HdGJ1V0MBhu",
    "i8dCvX2FUVCtc33UOUsC6X",
    "af8YE2XksKREuOXQ3oTIo3",
    "Oc1C0ic0YHkUciGYgjyRjK",
    "5ow5Tlml6lfZ3EGT5qynZD",
    "C4IIf8tn0v64Q37yWDnID1",
    "Ccy0LGz2ONouw7zltn3OLi",
    "nxd0g5wyPfkEIUczifrLko",
    "g9udcGq4dzmVv77k6JmOI0",
    "veDQii1FzqZqY3IkrxaKb9",
    "9CmddA2VknKlK16BAEmKTn",
    "xBTsnO8XI9cJgn3ICHdFuz",
    "d7ZpOwi7zAYFth2VG3ogfF",
    "uKxJXxIuQdx53DPlDvxnit",
    "UfIJQW2qBVF8fQT4mzymXx",
    "2fUwZqFtMySJf5UEnnH0Dv",
    "iutIQnOD3H8wU5LhyOlngJ",
    "6xZvPF9BbeEc2k7vDs6TGd",
    "3PooltVifT93LMj1Iw8f6N",
    "vb98CBmeXwkDpsnV3t8mqz",
    "SfsCs9xZEL3cCgKmTf2BPj",
    "lcwF7Tk6Y2SUHQcOycCJft",
    "WnbGgzXbeKkRqmMUM7YmnS",
    "NRC03cJzisqn9l77O7dOcZ",
    "t8ZPtL3F0zfEUE8bLoBERx",
    "EiUnODpIhI44h5LBoZVmhS",
    "sMvDzumUJMgZ5SdpYf6SPX",
    "GHr5m7gPmPIjbNXtOO1HoT",
    "ijpljzGKj8Uwp89epPVUza",
    "GT6PmhS2hWnloBZ3exnPYS",
    "hkTSjG7hXoqC2L31VsYpMh",
    "qcqZIEdxND6rkKidIptueN",
    "lnBJjpikNpTXJYWyADeScN",
    "4eZYapPGLduFkX1THmhzdn",
    "rZoq0fe5jzBT8D7m7gGz7z",
    "iT20a5sqM0NI05AMb20gOf",
    "QzVsfTqHqWfB9fRKHDBsy1",
    "88s8rK5YueO1OGZ0B9XJn3",
    "8mthDba9nY61ovTwDnNlEr",
    "99wUJg8F2LnoPlhP6j4yra",
    "uQwQXAYWlCWjnFz3h1hoOG",
    "e0z5VNLhIp3tgA3n7WoSI0",
    "IDgbk25rIvUHbM47iQM873",
    "Z3wCsJJGT1HrRsbgGAblYa",
    "Su3Vm1Q5EaylJBSEHTAzsv",
    "c4VdWh8D9UXYIF05Z6QwKw",
    "dWm98UQIhNYCylP4Uw1AOe",
    "szSbc3oi6BQNrmYzZBabX1",
    "BU4QQSNkpiTkHBhLOp8hby",
    "LFrGh4PvdcFo62ck9HZTkK",
    "uR4HTvPNh8jKT1fJZPuKww",
    "AKAXzjtvIy8DRgbwJ03zxv",
    "Q6LNdMvt2HKFWZPdIQE7vT",
    "vkWaiVHRDM3VjQYvW5ChM8",
    "Mi1FIeIKuFQdqgboiVITIs",
    "7mR071PWTm2VnlcE8mjyes",
    "o6craFalVsxJmBL3nvq2wm",
    "2Nzgmc14i0niJO5iuY5mXu",
    "CLFHGxwqJSzIfDT7uEx9v1",
    "jMiRVcNPk0FiUM5FVhfmvl",
    "etfSnF1NvgmrT9rcQZDBSP",
    "5RFRS9Ppjdhu1ZysI2uCcn",
    "EhmAdenAy536jEojbZyxEa",
    "hxKsszGPIsGO5aYKVVa3Vp",
    "G4UmcWa55G8lXCpnXs5vfT",
    "pQmoRDZugR1ugtyBrxPUtM",
    "qHtSocgUQorAkg9FelISt6",
    "I07fLRnehsBg1tRm1XXTa8",
    "GYkFP59kH6vPeOKoQ6NQsV"
   ],
   "version": 3
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
